{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f303d0b",
   "metadata": {},
   "source": [
    "# ü§óüîç Basic RAG using Hugging Face Embeddings + LLM\n",
    "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) pipeline using Hugging Face models for both embeddings and generation.\n",
    "We'll use LangChain components to wire the system together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b8f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from `.env` file for Hugging Face authentication\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36afbc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c09efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chirag/miniconda3/envs/LLMs-env/lib/python3.10/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "# Load PDF and split it into chunks\n",
    "pdf_file = 'sample.pdf'\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "loader = PyPDFLoader(pdf_file)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75f05c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 19.3 (Macintosh)', 'creationdate': '2024-06-18T14:09:48-07:00', 'moddate': '2024-06-18T14:10:14-07:00', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='Before using iPhone, review the iPhone User Guide  at  \\nsupport.apple.com/guide/iphone .\\nSafety and Handling\\nSee ‚ÄúSafety, handling, and support‚Äù in the iPhone  \\nUser Guide .\\nExposure to Radio Frequency\\nOn iPhone, go to Settings > General > Legal &  \\nRegulatory > RF Exposure. Or go to apple.com/  \\nlegal/rfexposure .\\nBattery and Charging\\nAn iPhone battery should only be repaired by a trained \\ntechnician to avoid battery damage, which could cause \\noverheating, fire, or injury. Batteries should be recycled \\nor disposed of separately from household waste and \\naccording to local environmental laws and guidelines. For \\ninformation about Apple lithium-ion batteries and battery \\nservice and recycling, go to apple.com/batteries/service-\\nand-recycling . For information about charging, see \\n‚ÄúImportant safety information‚Äù in the iPhone User Guide.\\nLasers\\nThe proximity sensor, the TrueDepth camera system, \\nand the LiDAR Scanner contain one or more lasers.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bda47be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c24c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110dd617b1e64762b30fd8b584afb106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e669229e5d2b45b496b09fd282b16d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8a564cd4a54e87914aa4089e78eded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c12fa309a04606891f7491d06bb9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8b7b02f79043c089588845b2d8d207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b7b873b3f84e8c9833c2c89301086f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0345c7b62a194c4eb3dd6bd238214ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2318362beba94dfe8fef568d5c47dd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d435378dbb84226923c76e8ceed4fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fca6ac0106b4b4b8f632983f1d1ec65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbb2885fe284b059d0b363141f913af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üîß Specify the Hugging Face embedding model to use\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# ‚öôÔ∏è Model-specific arguments (e.g., run on CPU)\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "# üßÆ Embedding behavior settings\n",
    "# normalize_embeddings=False ensures raw embeddings are used (not L2-normalized)\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# ü§ó Initialize the HuggingFaceEmbeddings class with specified settings\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1bfdb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store from the embedded chunks\n",
    "vectorstore = FAISS.from_documents(\n",
    "    texts,\n",
    "    embedding=hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39572507",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom prompt template for generation\n",
    "\n",
    "template = \"\"\"\n",
    "You are a customer service agent for a apple mobile company. \n",
    "You have been given the following information about the customer query and the context.\n",
    "Customer Query: {query}\n",
    "Context: {context}\n",
    "\n",
    "### Answer: \n",
    "The answer should be based on the context provided.\n",
    "Your task is to answer the customer query based on the context provided. If the question is not related to the context, please say \"I don't know or Do Not Answer it just say please ask me question related to Apple Mobiles only\".\n",
    "Do not make up any information or provide any personal opinions or experiences.\n",
    "Please answer in a friendly and professional manner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae9b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'query'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, template='\\nYou are a customer service agent for a apple mobile company. \\nYou have been given the following information about the customer query and the context.\\nCustomer Query: {query}\\nContext: {context}\\n\\nAnswer: \\nThe answer should be based on the context provided.\\nYour task is to answer the customer query based on the context provided. If the question is not related to the context, please say \"I don\\'t know or Do Not Answer it just say please ask me question related to Apple Mobiles only\".\\nDo not make up any information or provide any personal opinions or experiences.\\nPlease answer in a friendly and professional manner.\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Wrap prompt inside ChatPromptTemplate for compatibility with chat models\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t(\"system\", \"You are a helpful assistant.\"),\n",
    "\t(\"human\", template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164b404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc689fc0f964877bf85400add21619d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Authenticate with Hugging Face Hub\n",
    "from huggingface_hub import login\n",
    "login(os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Load a Hugging Face LLM for text generation using Transformers and wrap it with LangChain\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# üß† Model ID from Hugging Face Hub (you can replace this with any causal LLM)\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "# üóùÔ∏è Load the tokenizer for the chosen model (handles input formatting and tokenization)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# üß± Load the actual causal language model (Gemma-2B-IT in this case)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# üõ†Ô∏è Create a text-generation pipeline from the loaded model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",        # task type\n",
    "    model=model,              # pre-trained model\n",
    "    tokenizer=tokenizer,      # matching tokenizer\n",
    "    max_new_tokens=512,       # output token limit\n",
    "    do_sample=True,           # enables sampling (vs deterministic greedy decoding)\n",
    "    temperature=0.7,          # sampling randomness (lower = more focused)\n",
    "    top_p=0.95,               # nucleus sampling cutoff\n",
    "    repetition_penalty=1.1    # discourages repeating phrases\n",
    ")\n",
    "\n",
    "# üîÅ Wrap the Hugging Face pipeline in LangChain's HuggingFacePipeline for integration\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine content of all retrieved documents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c5ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom parser to extract clean answer from model output\n",
    "class CleanAnswerParser(StrOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        # Only keep the part after '### Answer:'\n",
    "        if \"### Answer:\" in text:\n",
    "            return text.split(\"### Answer:\")[-1].strip().split(\"\\n\")[0]\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    \n",
    "    | CleanAnswerParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701cbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Answer: Hello! Yes, there is a warranty on your phone. Apple provides a one-year limited warranty for the included hardware product and accessories against defects in materials and workmanship from the date of original retail purchase. Please note that the warranty does not cover normal wear and tear, or damage caused by accident or abuse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Ask a domain-specific question\n",
    "response = rag_chain.invoke('Is there a warranty on the phone?')\n",
    "print('üì¶ Answer:', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73833195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Ask a general knowledge question\n",
    "response = rag_chain.invoke('What is GTA 6?')\n",
    "print('üéÆ Answer:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f360c",
   "metadata": {},
   "source": [
    "<!-- Font Awesome CDN (Add in <head> if not already included) -->\n",
    "<link\n",
    "  rel=\"stylesheet\" \n",
    "  href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css\"\n",
    "/>\n",
    "\n",
    "<!-- Social Footer Section -->\n",
    "<div style=\"\n",
    "  background-color:rgb(199, 195, 195);\n",
    "  padding: 40px 30px;\n",
    "  border-radius: 20px;\n",
    "  box-shadow: 0 4px 12px rgba(0,0,0,0.08);\n",
    "  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "  font-size: 18px;\n",
    "  max-width: 900px;\n",
    "  margin: 60px auto 30px;\n",
    "  text-align: center;\n",
    "  color: #444;\n",
    "\">\n",
    "<!-- End of Notebook Note -->\n",
    "  <h2 style=\"margin-bottom: 10px;\">üìò End of Notebook</h2>\n",
    "  <p style=\"color: #666; font-size: 14px;\">\n",
    "    Thank you for exploring! Feel free to connect via the links below.\n",
    "  </p>\n",
    "\n",
    "  <!-- Social Icons -->\n",
    "<div style=\"\n",
    "  display: flex;\n",
    "  gap: 25px;\n",
    "  align-items: center;\n",
    "  flex-wrap: wrap;\n",
    "  justify-content: center;\n",
    "  margin-bottom: 25px;\n",
    "\">\n",
    "  <!-- LinkedIn -->\n",
    "  <a href=\"https://www.linkedin.com/in/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #0077b5;\">\n",
    "    <i class=\"fab fa-linkedin fa-lg\"></i> LinkedIn\n",
    "  </a>\n",
    "\n",
    "  <!-- GitHub -->\n",
    "  <a href=\"https://github.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #333;\">\n",
    "    <i class=\"fab fa-github fa-lg\"></i> GitHub\n",
    "  </a>\n",
    "\n",
    "  <!-- Instagram -->\n",
    "  <a href=\"https://www.instagram.com/data.scientist_chirag\" target=\"_blank\" style=\"text-decoration: none; color: #E1306C;\">\n",
    "    <i class=\"fab fa-instagram fa-lg\"></i> Instagram\n",
    "  </a>\n",
    "\n",
    "  <!-- Email -->\n",
    "  <a href=\"mailto:devchirag27@gmail.com\" style=\"text-decoration: none; color: #D44638;\">\n",
    "    <i class=\"fas fa-envelope fa-lg\"></i> Email\n",
    "  </a>\n",
    "\n",
    "  <!-- X (Twitter) -->\n",
    "  <a href=\"https://x.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #000;\">\n",
    "    <i class=\"fab fa-x-twitter fa-lg\"></i> X.com\n",
    "  </a>\n",
    "  </div>\n",
    "\n",
    "  <p style=\"font-size: 13px; color: black; font-style: italic; margin-top: 8px;\">\n",
    "    <strong>Made with ‚ù§Ô∏è by Chirag Bansal</strong>\n",
    "  </p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
