{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c5f5fb",
   "metadata": {},
   "source": [
    "# üß† Retrieval-Augmented Generation (RAG) with Re-Ranker ‚Äì Explained Simply\n",
    "\n",
    "Welcome!  \n",
    "This notebook demonstrates how to build a **RAG (Retrieval-Augmented Generation)** system with a **re-ranker**. It's designed to help **AI models answer questions more accurately** by first finding relevant information from a document (like a PDF), then carefully selecting the **best parts** to base the answer on.\n",
    "\n",
    "### üß© What‚Äôs Happening Here?\n",
    "\n",
    "Think of it like this:\n",
    "1. You ask a question üí¨\n",
    "2. The computer looks inside a PDF to find helpful info üìÑ\n",
    "3. It uses a **smart filter (re-ranker)** to choose only the best info ‚úÖ\n",
    "4. Then it asks an AI (like ChatGPT) to answer the question based on that info ü§ñ\n",
    "\n",
    "### üîß What This Notebook Includes:\n",
    "\n",
    "- üì• **Document Ingestion**: Loading and breaking the PDF into smaller chunks\n",
    "- üîç **Retrieval**: Finding the most relevant pieces for a question\n",
    "- üèÖ **Re-Ranking**: Reordering the results so the best answers come first\n",
    "- üí¨ **Final Answer Generation**: Using a language model (LLM) to give a final answer based only on the filtered results\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Sections Breakdown\n",
    "\n",
    "### üìò 1. Imports\n",
    "Loading all the tools and libraries needed for processing, retrieval, and generation.\n",
    "\n",
    "### üìÑ 2. Load and Split PDF\n",
    "Read your PDF and break it into smaller readable pieces for the AI.\n",
    "\n",
    "### üß† 3. Vector Store with FAISS\n",
    "Convert the text into searchable numbers (embeddings) and save them in FAISS.\n",
    "\n",
    "### üìä 4. Reranker Model\n",
    "Load a smart model that ranks the best pieces of text to answer the question.\n",
    "\n",
    "### üìù 5. Prompt Setup\n",
    "Tell the AI how to answer based on the context it gets.\n",
    "\n",
    "### üöÄ 6. Full RAG Pipeline\n",
    "Connect everything together: retrieve ‚Üí rerank ‚Üí generate ‚Üí display answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24762af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Imports:\n",
    "# This part brings in all the tools we need.\n",
    "# - PyPDFLoader: Helps us read PDFs\n",
    "# - TextSplitter: Breaks large text into smaller, readable parts\n",
    "# - FAISS: Helps us search fast through text chunks\n",
    "# - Prompts and Chains: Used to build the conversation with the AI model\n",
    "# - LLMCall and Embeddings: These are your AI brain and text encoder\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from llm_call import LLMCall\n",
    "from embeddings import Embeddings\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877ba0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads a PDF file from your computer.\n",
    "# Then it breaks the PDF into small parts (called \"chunks\") so that the AI can read and understand them better.\n",
    "# Big texts confuse models ‚Äî this makes it easier!\n",
    "\n",
    "pdf_file = 'sample.pdf'\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "loader = PyPDFLoader(pdf_file)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "707f5fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 19.3 (Macintosh)', 'creationdate': '2024-06-18T14:09:48-07:00', 'moddate': '2024-06-18T14:10:14-07:00', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='Before using iPhone, review the iPhone User Guide  at  \\nsupport.apple.com/guide/iphone .\\nSafety and Handling\\nSee ‚ÄúSafety, handling, and support‚Äù in the iPhone  \\nUser Guide .\\nExposure to Radio Frequency\\nOn iPhone, go to Settings > General > Legal &  \\nRegulatory > RF Exposure. Or go to apple.com/  \\nlegal/rfexposure .\\nBattery and Charging\\nAn iPhone battery should only be repaired by a trained \\ntechnician to avoid battery damage, which could cause \\noverheating, fire, or injury. Batteries should be recycled \\nor disposed of separately from household waste and \\naccording to local environmental laws and guidelines. For \\ninformation about Apple lithium-ion batteries and battery \\nservice and recycling, go to apple.com/batteries/service-\\nand-recycling . For information about charging, see \\n‚ÄúImportant safety information‚Äù in the iPhone User Guide.\\nLasers\\nThe proximity sensor, the TrueDepth camera system, \\nand the LiDAR Scanner contain one or more lasers.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb4b886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d3160",
   "metadata": {},
   "source": [
    "## Re-Ranker Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads a second AI model (a re-ranker).\n",
    "# It takes the documents found from the FAISS database and scores them to find the best ones.\n",
    "# Think of it like ranking Google search results from most to least helpful.\n",
    "# It uses a pre-trained model from BAAI called \"bge-reranker-base\".\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load reranker model\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-reranker-base\")\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(\"BAAI/bge-reranker-base\")\n",
    "\n",
    "def rerank_documents(query: str, documents: list, top_k: int = 5) -> list:\n",
    "    # Convert to list of strings if needed\n",
    "    passages = [doc.page_content if hasattr(doc, \"page_content\") else str(doc) for doc in documents]\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = reranker_tokenizer(\n",
    "        [query] * len(passages),\n",
    "        passages,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = reranker_model(**inputs).logits.squeeze(-1)\n",
    "\n",
    "    # Sort by score\n",
    "    reranked = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "    top_docs = [doc for doc, _ in reranked[:top_k]]\n",
    "\n",
    "    return top_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Define the custom prompt template used in the final RAG stage\n",
    "# This sets up how we will ask the AI to answer our question.\n",
    "# It gives the AI instructions like:\n",
    "# \"Use this context to answer. If you don't know, say you don't know.\"\n",
    "# It helps keep answers accurate and based only on your PDF.\n",
    "\n",
    "rag_template = \"\"\"\n",
    "You are a customer service agent for a apple mobile company. \n",
    "You have been given the following information about the customer question and the context.\n",
    "Customer Query: {question}\n",
    "Context: {context}\n",
    "\n",
    "Answer: \n",
    "The answer should be based on the context provided.\n",
    "Your task is to answer the customer question based on the context provided. If the question is not related to the context, please say \"I don't know or Do Not Answer it just say please ask me question related to Apple Mobiles only\".\n",
    "Do not make up any information or provide any personal opinions or experiences.\n",
    "Please answer in a friendly and professional manner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d2d0cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nYou are a customer service agent for a apple mobile company. \\nYou have been given the following information about the customer question and the context.\\nCustomer Query: {question}\\nContext: {context}\\n\\nAnswer: \\nThe answer should be based on the context provided.\\nYour task is to answer the customer question based on the context provided. If the question is not related to the context, please say \"I don\\'t know or Do Not Answer it just say please ask me question related to Apple Mobiles only\".\\nDo not make up any information or provide any personal opinions or experiences.\\nPlease answer in a friendly and professional manner.\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t(\"system\", \"You are a helpful assistant.\"),\n",
    "\t(\"human\", rag_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b38112",
   "metadata": {},
   "source": [
    "# üìò Run the Full RAG Pipeline:\n",
    "\n",
    "-  This is the main part of the program.\n",
    " >1. It gets a question from the user.\n",
    " >2. It finds the most related text chunks using the retriever.\n",
    " >3. It re-ranks those chunks to bring the most useful ones to the top.\n",
    " >4. It sends those top chunks and the question to the AI model.\n",
    " >5. It prints out the answer from the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8088ff",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Using Azure OpenAI for Embeddings & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be56bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Initialize Azure OpenAI Embeddings\n",
    "open_ai_embeddings = Embeddings.azure_openai()\n",
    "\n",
    "vectorstore_openai = FAISS.from_documents(\n",
    "    texts,\n",
    "    open_ai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore_openai.as_retriever()\n",
    "\n",
    "# ü§ñ Initialize Azure OpenAI Chat Model (LLM)\n",
    "open_ai_llm = LLMCall.azure_openai()\n",
    "\n",
    "question = \"Is there a warranty on the phone?\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "reranked_docs = rerank_documents(question, retrieved_docs, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da9c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Answer: Yes, there is a warranty on the phone. Apple offers a One-Year Limited Warranty that covers defects in materials and workmanship for one year from the date of original retail purchase. However, this warranty does not cover normal wear and tear or damage caused by accident or abuse. If you need to obtain service under this warranty, you can call Apple or visit an Apple Store or an Apple Authorized Service Provider. Please remember that you may need to provide proof of purchase when making a claim. For more detailed information, you can visit apple.com/legal/warranty.\n"
     ]
    }
   ],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": lambda _: reranked_docs, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | open_ai_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = final_rag_chain.invoke({\"question\": question})\n",
    "print(\"üì¶ Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db46dc76",
   "metadata": {},
   "source": [
    "## ü§ó Using Hugging Face for Embeddings & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80259138",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_embeddings = Embeddings.huggingface()\n",
    "\n",
    "vectorstore_hf = FAISS.from_documents(\n",
    "    texts,\n",
    "    huggingface_embeddings)\n",
    "\n",
    "retriever_hf = vectorstore_hf.as_retriever()\n",
    "\n",
    "huggingface_llm = LLMCall.huggingface()\n",
    "\n",
    "question = \"Is there a warranty on the phone?\"\n",
    "\n",
    "retrieved_docs = retriever_hf.invoke(question)\n",
    "reranked_docs = rerank_documents(question, retrieved_docs, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": lambda _: reranked_docs, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | huggingface_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = final_rag_chain.invoke({\"question\": question})\n",
    "print(\"üì¶ Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cb5ea7",
   "metadata": {},
   "source": [
    "## ü¶ô Using Ollama for Local LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd901610",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_llm = LLMCall.chat_ollama()\n",
    "\n",
    "vectorstore_ollama = FAISS.from_documents(\n",
    "    texts,\n",
    "    huggingface_embeddings)\n",
    "\n",
    "retriever_ollama = vectorstore_ollama.as_retriever()\n",
    "\n",
    "question = \"Is there a warranty on the phone?\"\n",
    "\n",
    "retrieved_docs = retriever_ollama.invoke(question)\n",
    "reranked_docs = rerank_documents(question, retrieved_docs, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": lambda _: reranked_docs, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | ollama_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = final_rag_chain.invoke({\"question\": question})\n",
    "print(\"üì¶ Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e9931",
   "metadata": {},
   "source": [
    "## ‚ö° Using Groq Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4024b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = LLMCall.chat_groq()\n",
    "\n",
    "vectorstore_groq = FAISS.from_documents(\n",
    "    texts,\n",
    "    # huggingface_embeddings\n",
    "    open_ai_embeddings)\n",
    "\n",
    "retriever_groq = vectorstore_groq.as_retriever()\n",
    "\n",
    "question = \"Is there a warranty on the phone?\"\n",
    "\n",
    "retrieved_docs = retriever_groq.invoke(question)\n",
    "reranked_docs = rerank_documents(question, retrieved_docs, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74df0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Answer: Yes, there is a warranty on the phone. According to the Apple One-Year Limited Warranty, the company warrants the included hardware product and accessories against defects in materials and workmanship for one year from the date of original retail purchase. However, please note that this warranty does not cover normal wear and tear, damage caused by accident or abuse. If you have any issues with your device, you can call Apple or visit an Apple Store or an Apple Authorized Service Provider for service. You may be required to furnish proof of purchase details when making a claim under this warranty. For more detailed information, you can visit apple.com/legal/warranty and support.apple.com.\n"
     ]
    }
   ],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": lambda _: reranked_docs, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | groq_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = final_rag_chain.invoke({\"question\": question})\n",
    "print(\"üì¶ Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9c25e",
   "metadata": {},
   "source": [
    "<!-- Font Awesome CDN (Add in <head> if not already included) -->\n",
    "<link\n",
    "  rel=\"stylesheet\" \n",
    "  href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css\"\n",
    "/>\n",
    "\n",
    "<!-- Social Footer Section -->\n",
    "<div style=\"\n",
    "  background-color:rgb(199, 195, 195);\n",
    "  padding: 40px 30px;\n",
    "  border-radius: 20px;\n",
    "  box-shadow: 0 4px 12px rgba(0,0,0,0.08);\n",
    "  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "  font-size: 18px;\n",
    "  max-width: 900px;\n",
    "  margin: 60px auto 30px;\n",
    "  text-align: center;\n",
    "  color: #444;\n",
    "\">\n",
    "<!-- End of Notebook Note -->\n",
    "  <h2 style=\"margin-bottom: 10px;\">üìò End of Notebook</h2>\n",
    "  <p style=\"color: #666; font-size: 14px;\">\n",
    "    Thank you for exploring! Feel free to connect via the links below.\n",
    "  </p>\n",
    "\n",
    "  <!-- Social Icons -->\n",
    "<div style=\"\n",
    "  display: flex;\n",
    "  gap: 25px;\n",
    "  align-items: center;\n",
    "  flex-wrap: wrap;\n",
    "  justify-content: center;\n",
    "  margin-bottom: 25px;\n",
    "\">\n",
    "  <!-- LinkedIn -->\n",
    "  <a href=\"https://www.linkedin.com/in/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #0077b5;\">\n",
    "    <i class=\"fab fa-linkedin fa-lg\"></i> LinkedIn\n",
    "  </a>\n",
    "\n",
    "  <!-- GitHub -->\n",
    "  <a href=\"https://github.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #333;\">\n",
    "    <i class=\"fab fa-github fa-lg\"></i> GitHub\n",
    "  </a>\n",
    "\n",
    "  <!-- Instagram -->\n",
    "  <a href=\"https://www.instagram.com/data.scientist_chirag\" target=\"_blank\" style=\"text-decoration: none; color: #E1306C;\">\n",
    "    <i class=\"fab fa-instagram fa-lg\"></i> Instagram\n",
    "  </a>\n",
    "\n",
    "  <!-- Email -->\n",
    "  <a href=\"mailto:devchirag27@gmail.com\" style=\"text-decoration: none; color: #D44638;\">\n",
    "    <i class=\"fas fa-envelope fa-lg\"></i> Email\n",
    "  </a>\n",
    "\n",
    "  <!-- X (Twitter) -->\n",
    "  <a href=\"https://x.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #000;\">\n",
    "    <i class=\"fab fa-x-twitter fa-lg\"></i> X.com\n",
    "  </a>\n",
    "  </div>\n",
    "\n",
    "  <p style=\"font-size: 13px; color: black; font-style: italic; margin-top: 8px;\">\n",
    "    <strong>Made with ‚ù§Ô∏è by Chirag Bansal</strong>\n",
    "  </p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
