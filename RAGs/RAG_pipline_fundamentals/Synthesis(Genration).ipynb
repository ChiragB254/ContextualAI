{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d21888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Load environment variables from a .env file\n",
    "This script is used to load environment variables from a .env file\n",
    "into the environment so that they can be accessed by the application.\n",
    "This is useful for managing sensitive information such as API keys,\n",
    "database credentials, and other configuration settings.\n",
    "'''\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97e94f",
   "metadata": {},
   "source": [
    "## üß† **User Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input\n",
    "query = \"Is it charge any international charges?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001729e3",
   "metadata": {},
   "source": [
    "### VecDB and Retriever Part same as previous parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de2ff2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import faiss\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c35226fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "\tazure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    \tazure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"],\n",
    "    \topenai_api_version=os.environ[\"AZURE_OPENAI_EMBEDDING_API_VERSION\"],\n",
    "    \tmodel=\"text-embedding-3-small\",\n",
    "\tchunk_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8ee20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = faiss.FAISS.load_local(\"my_faiss_index\", embeddings ,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23105eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0844756",
   "metadata": {},
   "source": [
    "## üß± **Context Assembly**\n",
    "\n",
    "This step combines the relevant retrieved chunks into a single prompt context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Retrieve relevant documents\n",
    "This function retrieves relevant documents based on the user's query\n",
    "Same as we did in the Retriever.ipynb notebook\n",
    "'''\n",
    "contexts = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Combine the retrieved documents into a single context\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in contexts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540cd46",
   "metadata": {},
   "source": [
    "## üìù **Prompt Formatting**\n",
    "\n",
    "The context and user query are formatted into a structured prompt for the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Chat Prompt Template from LangChain\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf83e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a prompt which mimic the customer service agent\n",
    "The prompt is designed to provide a friendly and professional response\n",
    "to the customer's query based on the context provided.\n",
    "\n",
    "The prompt includes the customer query and the context\n",
    "to ensure that the response is relevant and accurate.\n",
    "\n",
    "This prompt is variable and can be changed based on the requirements of the application \n",
    "for which are trying to build a RAG application.\n",
    "'''\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a customer service agent for a mobile phone company. You have been given the following information about the customer query and the context.\n",
    "Customer Query: {query}\n",
    "Context: {context}\n",
    "\n",
    "Answer: \n",
    "Your task is to answer the customer query based on the context provided.\n",
    "Please answer in a friendly and professional manner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a customer service agent for a mobile phone company. You have been given the following information about the customer query and the context.\n",
      "Customer Query: Is it charge any international charges?\n",
      "Context: and may be restricted to the original country of sale. Call \n",
      "charges and international shipping charges may apply, \n",
      "depending on the location. Subject to the full terms and \n",
      "detailed information on obtaining service available at \n",
      "apple.com/legal/warranty  and support.apple.com , if yo\n"
     ]
    }
   ],
   "source": [
    "# Create a ChatPromptTemplate object\n",
    "# Here I am adding the context and the query to the prompt, these two things are dynamic for each query\n",
    "# and will be passed to the prompt at runtime.\n",
    "final_prompt = prompt.format(context=context, query=query)\n",
    "print(final_prompt[:500]+\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cc7a2b",
   "metadata": {},
   "source": [
    "## ü§ñ **Language Model Input**\n",
    "\n",
    "The final prompt is passed to the LLM for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a93ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the ChatOpenAI class from LangChain\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f88ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this example, we are using the AzureChatOpenAI class to create a chat model\n",
    "that can be used to generate responses to customer queries.\n",
    "The AzureChatOpenAI class is a wrapper around the OpenAI API\n",
    "that allows us to use the Azure OpenAI service.\n",
    "'''\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "\tazure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "\topenai_api_version=os.environ[\"AZURE_OPENAI_CHAT_API_VERSION\"],\n",
    "\ttemperature=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60928572",
   "metadata": {},
   "source": [
    "## üß† **Response Generation**\n",
    "\n",
    "The language model returns an answer based on the given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54784248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/39/wh_jffd90vv2lczbpfngvr200000gn/T/ipykernel_40419/3024218951.py:2: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm([HumanMessage(content=final_prompt)])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Generate a response using the chat model\n",
    "The response is generated based on the prompt provided\n",
    "and the chat model used.\n",
    "'''\n",
    "from langchain.schema import HumanMessage\n",
    "response = llm([HumanMessage(content=final_prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "215425e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for reaching out! Regarding your question about international charges, it‚Äôs important to note that call charges and international shipping charges may apply depending on your location. If you‚Äôre looking to make calls or use services that incur international fees, those charges will vary based on your specific plan and destination. \n",
      "\n",
      "For the most accurate information, I recommend checking your mobile plan details or contacting your service provider directly. If you have any more questions or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8089e8a5",
   "metadata": {},
   "source": [
    "## üì§ **Answer Output (Post-processing)**\n",
    "\n",
    "The final answer is displayed to the user. It can be returned in chat, UI, or API response, but here we are just print it using print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b47612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final Answer:\n",
      " Thank you for reaching out! Regarding your question about international charges, it‚Äôs important to note that call charges and international shipping charges may apply depending on your location. If you‚Äôre looking to make calls or use services that incur international fees, those charges will vary based on your specific plan and destination. \n",
      "\n",
      "For the most accurate information, I recommend checking your mobile plan details or contacting your service provider directly. If you have any more questions or need further assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "answer = response.content\n",
    "print(\"\\n‚úÖ Final Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1af3cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<!-- Font Awesome CDN (Add in <head> if not already included) -->\n",
    "<link\n",
    "  rel=\"stylesheet\"\n",
    "  href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css\"\n",
    "/>\n",
    "\n",
    "<!-- Social Footer Section -->\n",
    "<div style=\"\n",
    "  background-color:rgb(199, 195, 195);\n",
    "  padding: 40px 30px;\n",
    "  border-radius: 20px;\n",
    "  box-shadow: 0 4px 12px rgba(0,0,0,0.08);\n",
    "  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "  font-size: 18px;\n",
    "  max-width: 900px;\n",
    "  margin: 60px auto 30px;\n",
    "  text-align: center;\n",
    "  color: #444;\n",
    "\">\n",
    "<!-- End of Notebook Note -->\n",
    "  <h2 style=\"margin-bottom: 10px;\">üìò End of Notebook</h2>\n",
    "  <p style=\"color: #666; font-size: 14px;\">\n",
    "    Thank you for exploring! Feel free to connect via the links below.\n",
    "  </p>\n",
    "\n",
    "  <!-- Social Icons -->\n",
    "<div style=\"\n",
    "  display: flex;\n",
    "  gap: 25px;\n",
    "  align-items: center;\n",
    "  flex-wrap: wrap;\n",
    "  justify-content: center;\n",
    "  margin-bottom: 25px;\n",
    "\">\n",
    "  <!-- LinkedIn -->\n",
    "  <a href=\"https://www.linkedin.com/in/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #0077b5;\">\n",
    "    <i class=\"fab fa-linkedin fa-lg\"></i> LinkedIn\n",
    "  </a>\n",
    "\n",
    "  <!-- GitHub -->\n",
    "  <a href=\"https://github.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #333;\">\n",
    "    <i class=\"fab fa-github fa-lg\"></i> GitHub\n",
    "  </a>\n",
    "\n",
    "  <!-- Instagram -->\n",
    "  <a href=\"https://www.instagram.com/data.scientist_chirag\" target=\"_blank\" style=\"text-decoration: none; color: #E1306C;\">\n",
    "    <i class=\"fab fa-instagram fa-lg\"></i> Instagram\n",
    "  </a>\n",
    "\n",
    "  <!-- Email -->\n",
    "  <a href=\"mailto:devchirag27@gmail.com\" style=\"text-decoration: none; color: #D44638;\">\n",
    "    <i class=\"fas fa-envelope fa-lg\"></i> Email\n",
    "  </a>\n",
    "\n",
    "  <!-- X (Twitter) -->\n",
    "  <a href=\"https://x.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #000;\">\n",
    "    <i class=\"fab fa-x-twitter fa-lg\"></i> X.com\n",
    "  </a>\n",
    "  </div>\n",
    "\n",
    "  <p style=\"font-size: 13px; color: black; font-style: italic; margin-top: 8px;\">\n",
    "    <strong>Made with ‚ù§Ô∏è by Chirag Bansal</strong>\n",
    "  </p>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
