{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02a7d9e",
   "metadata": {},
   "source": [
    "# üîÄ RAG Fusion with Multiple LLM Backends\n",
    "\n",
    "This notebook implements **RAG Fusion**, an enhancement of standard Retrieval-Augmented Generation (RAG) where **multiple sub-queries** are used to retrieve a more diverse and complete set of context documents.\n",
    "\n",
    "We fuse the results from each sub-query using **Reciprocal Rank Fusion (RRF)** and pass the unified context to the LLM to generate an accurate answer.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß LLMs & Embeddings Used:\n",
    "\n",
    "- **Embeddings:** `sentence-transformers/all-mpnet-base-v2` (Hugging Face)\n",
    "- **LLMs:**\n",
    "  - üß† Azure OpenAI (`gpt-35-turbo` or `gpt-4`)\n",
    "  - ü§ó Hugging Face (`gemma-2b-it`)\n",
    "  - ü¶ô Ollama (locally run `llama3.2`)\n",
    "  - ‚ö° Groq Inference API (`llama-3.3-70b-versatile`)\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Highlights:\n",
    "- Sub-query generation improves recall\n",
    "- RRF scoring re-ranks retrieved chunks from multiple queries\n",
    "- You can switch between LLMs by changing one line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41819f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from llm_call import LLMCall\n",
    "from embeddings import Embeddings\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afc86e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF and split it into chunks\n",
    "pdf_file = 'sample.pdf'\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "loader = PyPDFLoader(pdf_file)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bb664a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 19.3 (Macintosh)', 'creationdate': '2024-06-18T14:09:48-07:00', 'moddate': '2024-06-18T14:10:14-07:00', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='Before using iPhone, review the iPhone User Guide  at  \\nsupport.apple.com/guide/iphone .\\nSafety and Handling\\nSee ‚ÄúSafety, handling, and support‚Äù in the iPhone  \\nUser Guide .\\nExposure to Radio Frequency\\nOn iPhone, go to Settings > General > Legal &  \\nRegulatory > RF Exposure. Or go to apple.com/  \\nlegal/rfexposure .\\nBattery and Charging\\nAn iPhone battery should only be repaired by a trained \\ntechnician to avoid battery damage, which could cause \\noverheating, fire, or injury. Batteries should be recycled \\nor disposed of separately from household waste and \\naccording to local environmental laws and guidelines. For \\ninformation about Apple lithium-ion batteries and battery \\nservice and recycling, go to apple.com/batteries/service-\\nand-recycling . For information about charging, see \\n‚ÄúImportant safety information‚Äù in the iPhone User Guide.\\nLasers\\nThe proximity sensor, the TrueDepth camera system, \\nand the LiDAR Scanner contain one or more lasers.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35fa1230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03bb3c6",
   "metadata": {},
   "source": [
    "## üß† RAG Fusion Prompt Template\n",
    "\n",
    "In RAG Fusion, we enhance the retrieval step by issuing multiple semantically varied versions of a user's question.  \n",
    "This helps the system fetch a broader and more accurate set of documents across different sub-query perspectives.\n",
    "\n",
    "### üîç Why This Matters:\n",
    "- Vector similarity search can be overly literal ‚Äî rephrased queries help surface diverse content\n",
    "- Reciprocal Rank Fusion (RRF) benefits from having varied but meaningful results from different queries\n",
    "- It reduces overfitting to a single phrasing and increases the robustness of document retrieval\n",
    "\n",
    "This template is used to instruct the LLM to generate **5 alternative versions** of the user‚Äôs input query.  \n",
    "Each version expresses the **same intent**, but uses different structure, keywords, or phrasing ‚Äî ideal for feeding into a multi-query retrieval pipeline, which is later fused via RRF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2d6d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_fusion_template = \"\"\"\n",
    "\n",
    "You are a question(query) generator. You can create multiple questions similar with multiple prespective to the given question with the same meaning.\n",
    "By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Please generate 5 questions similar to the given question.\n",
    "Make sure to use different words and phrases to express the same idea.\n",
    "The questions should be clear and concise.\n",
    "The questions should be grammatically correct and easy to understand.\n",
    "The questions should be in English.\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "The question is: {question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93b49c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_fusion_prompt = ChatPromptTemplate.from_messages(    [\n",
    "\t(\"system\", \"You are a helpful assistant.\"),\n",
    "\t(\"human\", rag_fusion_template),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71a8db",
   "metadata": {},
   "source": [
    "## üîÅ Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "RRF is used to merge results from multiple ranked retrievals (from sub-queries).\n",
    "It improves document diversity and recall, especially when different phrasings produce different top-k results.\n",
    "\n",
    "The formula used:  \n",
    "**Score = 1 / (rank + k)**\n",
    "\n",
    "Where:\n",
    "- `rank` = position of document in list\n",
    "- `k` = a constant (default = 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc7af4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e60d6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Define the custom prompt template used in the final RAG stage\n",
    "\n",
    "rag_template = \"\"\"\n",
    "You are a customer service agent for a apple mobile company. \n",
    "You have been given the following information about the customer question and the context.\n",
    "Customer Query: {question}\n",
    "Context: {context}\n",
    "\n",
    "Answer: \n",
    "The answer should be based on the context provided.\n",
    "Your task is to answer the customer question based on the context provided. If the question is not related to the context, please say \"I don't know or Do Not Answer it just say please ask me question related to Apple Mobiles only\".\n",
    "Do not make up any information or provide any personal opinions or experiences.\n",
    "Please answer in a friendly and professional manner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f1a980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nYou are a customer service agent for a apple mobile company. \\nYou have been given the following information about the customer question and the context.\\nCustomer Query: {question}\\nContext: {context}\\n\\nAnswer: \\nThe answer should be based on the context provided.\\nYour task is to answer the customer question based on the context provided. If the question is not related to the context, please say \"I don\\'t know or Do Not Answer it just say please ask me question related to Apple Mobiles only\".\\nDo not make up any information or provide any personal opinions or experiences.\\nPlease answer in a friendly and professional manner.\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t(\"system\", \"You are a helpful assistant.\"),\n",
    "\t(\"human\", rag_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459e2c73",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Using Azure OpenAI for Embeddings & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871531ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Initialize Azure OpenAI Embeddings\n",
    "open_ai_embeddings = Embeddings.azure_openai()\n",
    "\n",
    "vectorstore_openai = FAISS.from_documents(\n",
    "    texts,\n",
    "    open_ai_embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore_openai.as_retriever()\n",
    "\n",
    "# ü§ñ Initialize Azure OpenAI Chat Model (LLM)\n",
    "open_ai_llm = LLMCall.azure_openai()\n",
    "\n",
    "generate_queries = (\n",
    "    rag_fusion_prompt \n",
    "    | open_ai_llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "question = \"Is there a warranty on the phone?\"\n",
    "\n",
    "# üîÑ Build RAG Fusion Retrieval Chain\n",
    "# 1. Generate sub-queries from original question\n",
    "# 2. Run document retrieval for each query\n",
    "# 3. Combine and re-rank results using Reciprocal Rank Fusion (RRF)\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617aa7d5",
   "metadata": {},
   "source": [
    "## ü§ñ Azure OpenAI ‚Äì Final Answer Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0121435e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, there is a warranty on the phone. Apple provides a one-year limited warranty that covers defects in materials and workmanship for the included hardware product and accessories, starting from the date of original retail purchase. Please note that this warranty does not cover normal wear and tear or damage caused by accidents or abuse. For more detailed information on obtaining service, you can visit apple.com/legal/warranty or support.apple.com. If you have any further questions, feel free to ask!'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | rag_prompt\n",
    "    | open_ai_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092ee7b",
   "metadata": {},
   "source": [
    "## ü§ó Using Hugging Face for Embeddings & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_embeddings = Embeddings.huggingface()\n",
    "\n",
    "vectorstore_hf = FAISS.from_documents(\n",
    "    texts,\n",
    "    huggingface_embeddings)\n",
    "\n",
    "retriever_hf = vectorstore_hf.as_retriever()\n",
    "\n",
    "huggingface_llm = LLMCall.huggingface()\n",
    "\n",
    "generate_queries_hf = (\n",
    "    rag_fusion_prompt \n",
    "    | huggingface_llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# üîÑ Build RAG Fusion Retrieval Chain\n",
    "# 1. Generate sub-queries from original question\n",
    "# 2. Run document retrieval for each query\n",
    "# 3. Combine and re-rank results using Reciprocal Rank Fusion (RRF)\n",
    "retrieval_chain_rag_fusion_hf = generate_queries_hf | retriever_hf.map() | reciprocal_rank_fusion\n",
    "\n",
    "docs_hf = retrieval_chain_rag_fusion_hf.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f38161",
   "metadata": {},
   "source": [
    "## ü§ó Hugging Face ‚Äì Final Answer Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d6fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain_hf = (\n",
    "    {\"context\": retrieval_chain_rag_fusion_hf, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | rag_prompt\n",
    "    | huggingface_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain_hf.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6664821",
   "metadata": {},
   "source": [
    "## ü¶ô Using Ollama for Local LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cdbab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_llm = LLMCall.chat_ollama()\n",
    "\n",
    "vectorstore_ollama = FAISS.from_documents(\n",
    "    texts,\n",
    "    huggingface_embeddings)\n",
    "\n",
    "retriever_ollama = vectorstore_ollama.as_retriever()\n",
    "\n",
    "generate_queries_ollama = (\n",
    "    rag_fusion_prompt \n",
    "    | ollama_llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# üîÑ Build RAG Fusion Retrieval Chain\n",
    "# 1. Generate sub-queries from original question\n",
    "# 2. Run document retrieval for each query\n",
    "# 3. Combine and re-rank results using Reciprocal Rank Fusion (RRF)\n",
    "retrieval_chain_rag_fusion_ollama = (\n",
    "    generate_queries_ollama \n",
    "    | retriever_ollama.map()\n",
    "    | reciprocal_rank_fusion\n",
    ")\n",
    "\n",
    "docs_ollama = retrieval_chain_rag_fusion_ollama.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94978f1",
   "metadata": {},
   "source": [
    "## ü¶ô Ollama ‚Äì Final Answer Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain_ollama = (\n",
    "    {\"context\": retrieval_chain_rag_fusion_ollama, \n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | ollama_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain_ollama.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559e8bc",
   "metadata": {},
   "source": [
    "## ‚ö° Using Groq Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bef0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = LLMCall.chat_groq()\n",
    "\n",
    "vectorstore_groq = FAISS.from_documents(\n",
    "    texts,\n",
    "    huggingface_embeddings)\n",
    "\n",
    "retriever_groq = vectorstore_groq.as_retriever()\n",
    "\n",
    "generate_queries_groq = (\n",
    "    rag_fusion_prompt \n",
    "    | groq_llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# üîÑ Build RAG Fusion Retrieval Chain\n",
    "# 1. Generate sub-queries from original question\n",
    "# 2. Run document retrieval for each query\n",
    "# 3. Combine and re-rank results using Reciprocal Rank Fusion (RRF)\n",
    "retrieval_chain_rag_fusion_groq = (\n",
    "    generate_queries_groq \n",
    "    | retriever_groq.map()\n",
    "    | reciprocal_rank_fusion\n",
    ")\n",
    "\n",
    "docs_groq = retrieval_chain_rag_fusion_groq.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39c9f9",
   "metadata": {},
   "source": [
    "## ‚ö° Groq Inference ‚Äì Final Answer Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain_groq = (\n",
    "    {\"context\": retrieval_chain_rag_fusion_groq, \n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | groq_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain_groq.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e46f8",
   "metadata": {},
   "source": [
    "<!-- Font Awesome CDN (Add in <head> if not already included) -->\n",
    "<link\n",
    "  rel=\"stylesheet\" \n",
    "  href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css\"\n",
    "/>\n",
    "\n",
    "<!-- Social Footer Section -->\n",
    "<div style=\"\n",
    "  background-color:rgb(199, 195, 195);\n",
    "  padding: 40px 30px;\n",
    "  border-radius: 20px;\n",
    "  box-shadow: 0 4px 12px rgba(0,0,0,0.08);\n",
    "  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "  font-size: 18px;\n",
    "  max-width: 900px;\n",
    "  margin: 60px auto 30px;\n",
    "  text-align: center;\n",
    "  color: #444;\n",
    "\">\n",
    "<!-- End of Notebook Note -->\n",
    "  <h2 style=\"margin-bottom: 10px;\">üìò End of Notebook</h2>\n",
    "  <p style=\"color: #666; font-size: 14px;\">\n",
    "    Thank you for exploring! Feel free to connect via the links below.\n",
    "  </p>\n",
    "\n",
    "  <!-- Social Icons -->\n",
    "<div style=\"\n",
    "  display: flex;\n",
    "  gap: 25px;\n",
    "  align-items: center;\n",
    "  flex-wrap: wrap;\n",
    "  justify-content: center;\n",
    "  margin-bottom: 25px;\n",
    "\">\n",
    "  <!-- LinkedIn -->\n",
    "  <a href=\"https://www.linkedin.com/in/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #0077b5;\">\n",
    "    <i class=\"fab fa-linkedin fa-lg\"></i> LinkedIn\n",
    "  </a>\n",
    "\n",
    "  <!-- GitHub -->\n",
    "  <a href=\"https://github.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #333;\">\n",
    "    <i class=\"fab fa-github fa-lg\"></i> GitHub\n",
    "  </a>\n",
    "\n",
    "  <!-- Instagram -->\n",
    "  <a href=\"https://www.instagram.com/data.scientist_chirag\" target=\"_blank\" style=\"text-decoration: none; color: #E1306C;\">\n",
    "    <i class=\"fab fa-instagram fa-lg\"></i> Instagram\n",
    "  </a>\n",
    "\n",
    "  <!-- Email -->\n",
    "  <a href=\"mailto:devchirag27@gmail.com\" style=\"text-decoration: none; color: #D44638;\">\n",
    "    <i class=\"fas fa-envelope fa-lg\"></i> Email\n",
    "  </a>\n",
    "\n",
    "  <!-- X (Twitter) -->\n",
    "  <a href=\"https://x.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #000;\">\n",
    "    <i class=\"fab fa-x-twitter fa-lg\"></i> X.com\n",
    "  </a>\n",
    "  </div>\n",
    "\n",
    "  <p style=\"font-size: 13px; color: black; font-style: italic; margin-top: 8px;\">\n",
    "    <strong>Made with ‚ù§Ô∏è by Chirag Bansal</strong>\n",
    "  </p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
