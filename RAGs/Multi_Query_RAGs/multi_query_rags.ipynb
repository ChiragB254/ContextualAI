{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0a02fd",
   "metadata": {},
   "source": [
    "# üîÅ Multi-Query RAGs - RAG 101\n",
    "\n",
    "**Multi-Query RAG** is an advanced retrieval strategy where a single user query is rephrased or expanded into multiple diverse sub-queries. This helps in surfacing a wider and more comprehensive set of relevant documents from the vector store.\n",
    "\n",
    "### ‚ú® Why Multi-Query RAG?\n",
    "\n",
    "- Mitigates sparse or ambiguous queries\n",
    "- Increases retrieval coverage\n",
    "- Improves generation quality by exposing the model to diverse information slices\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works ‚Äì Multi-Stage RAG Pipeline\n",
    "\n",
    "This workflow enhances context gathering and improves answer quality by combining multi-query expansion with staged RAG chaining:\n",
    "\n",
    "1. **üí¨ User Question**  \n",
    "   The process starts with a single input query from the user.\n",
    "\n",
    "2. **üîÄ Multi-Query Expansion RAG**  \n",
    "   The original query is passed into a RAG chain using a custom prompt that generates multiple rephrasings or sub-queries.\n",
    "\n",
    "3. **üîé Sub-Query Retrieval RAG**  \n",
    "   Each sub-query goes through its own retrieval chain. The system collects relevant document chunks for each sub-query and unions all the results.\n",
    "\n",
    "4. **ü§ñ Final Answer Generation RAG**  \n",
    "   The original question, along with the aggregated context from all sub-queries, is passed to the final RAG chain for synthesis and response generation.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Multi-Stage RAG Flow\n",
    "\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"multiquery_rag.png\"\n",
    "       alt=\"Markdown Monster icon\"\n",
    "       style=\"margin-right: 10px;\"\n",
    "       width=\"500\"\n",
    "       height=\"500\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b39fb",
   "metadata": {},
   "source": [
    "<!-- Mermaid JS -->\n",
    "<script type=\"module\">\n",
    "  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';\n",
    "  mermaid.initialize({ startOnLoad: true });\n",
    "</script>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e7bec",
   "metadata": {},
   "source": [
    "All the important imports from langchain. Additionaly, I have also created to mode `.py` file one is `llm_call.py` and `embeddings.py` the main function of these file is to make this notebook more efficient and more readable. The python fiel `llm_call.py` is contain all the ChatLLM calls like - openAI, Groq Infrance API, Ollama, Huggingface open source models too. It contain a Class `LLMCall` and under this class I defined all the funtion with respective to the model(organization) which I am using. To dive deep into this please check out the file. Moreover, In `embeddings.py` file as ane suggest it contains a Class `Embeddings` in which you will find both the `OpenAI Embeddings` as well as `HuggingFace Embeddings` which I used in this Notebook as I move forward. If you want to dive deep again please chek out `Embeddings.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc2504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from llm_call import LLMCall\n",
    "from embeddings import Embeddings\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ed422e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chirag/miniconda3/envs/LLMs-env/lib/python3.10/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "# Load PDF and split it into chunks\n",
    "pdf_file = 'sample.pdf'\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "loader = PyPDFLoader(pdf_file)\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into manageable chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb8aecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 17.0', 'creator': 'Adobe InDesign 19.3 (Macintosh)', 'creationdate': '2024-06-18T14:09:48-07:00', 'moddate': '2024-06-18T14:10:14-07:00', 'trapped': '/False', 'source': 'sample.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1'}, page_content='Before using iPhone, review the iPhone User Guide  at  \\nsupport.apple.com/guide/iphone .\\nSafety and Handling\\nSee ‚ÄúSafety, handling, and support‚Äù in the iPhone  \\nUser Guide .\\nExposure to Radio Frequency\\nOn iPhone, go to Settings > General > Legal &  \\nRegulatory > RF Exposure. Or go to apple.com/  \\nlegal/rfexposure .\\nBattery and Charging\\nAn iPhone battery should only be repaired by a trained \\ntechnician to avoid battery damage, which could cause \\noverheating, fire, or injury. Batteries should be recycled \\nor disposed of separately from household waste and \\naccording to local environmental laws and guidelines. For \\ninformation about Apple lithium-ion batteries and battery \\nservice and recycling, go to apple.com/batteries/service-\\nand-recycling . For information about charging, see \\n‚ÄúImportant safety information‚Äù in the iPhone User Guide.\\nLasers\\nThe proximity sensor, the TrueDepth camera system, \\nand the LiDAR Scanner contain one or more lasers.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5c06a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb11ea6",
   "metadata": {},
   "source": [
    "## üß† Multi-Query Prompt Template\n",
    "\n",
    "In a Multi-Query RAG system, we improve document retrieval by generating **multiple diverse versions** of a user's query. This is important because:\n",
    "\n",
    "- Lexical variation increases the likelihood of matching relevant documents\n",
    "- Similarity search can miss key chunks if the user's wording is too narrow\n",
    "- Rephrasing helps cover edge cases and boost recall\n",
    "\n",
    "This prompt is used to guide the LLM in producing **5 alternative phrasings** of the same question.  \n",
    "Each version maintains the original intent but varies in vocabulary, structure, or tone ‚Äî ideal for feeding into a retrieval pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ad2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Multi-Query Prompt Template for Generating Diverse Sub-Queries\n",
    "\n",
    "multi_query_template = \"\"\"\n",
    "\n",
    "You are a question(query) generator. You can create multiple questions similar with multiple prespective to the given question with the same meaning.\n",
    "By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Please generate 5 questions similar to the given question.\n",
    "Make sure to use different words and phrases to express the same idea.\n",
    "The questions should be clear and concise.\n",
    "The questions should be grammatically correct and easy to understand.\n",
    "The questions should be in English.\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "The question is: {question}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27bbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_query_prompt = ChatPromptTemplate.from_messages(    [\n",
    "\t(\"system\", \"You are a helpful assistant.\"),\n",
    "\t(\"human\", multi_query_template),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426f591",
   "metadata": {},
   "source": [
    "## üßæ `get_unique_union()` ‚Äì Deduplicating Retrieved Documents\n",
    "\n",
    "When multiple sub-queries are run in a Multi-Query RAG pipeline, there's a chance that some documents are retrieved by more than one sub-query.\n",
    "\n",
    "To avoid duplication in the final context, this function:\n",
    "\n",
    "1. **Flattens** the nested document list (from multiple retrievers)\n",
    "2. **Serializes** each document using `langchain.load.dumps`\n",
    "3. **Deduplicates** them using Python‚Äôs built-in `set`\n",
    "4. **Deserializes** back to `Document` objects\n",
    "\n",
    "This ensures only **unique context chunks** are passed into the final answer generation stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a45be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\"\n",
    "    üîÅ Unique Union of Retrieved Documents\n",
    "\n",
    "    This function takes a list of lists of LangChain `Document` objects,\n",
    "    flattens them into a single list, and returns only unique documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list[list[Document]]): Nested list of retrieved documents\n",
    "                                           from multiple sub-query RAGs.\n",
    "\n",
    "    Returns:\n",
    "        list[Document]: Deduplicated list of Document objects\n",
    "    \"\"\"\n",
    "    # üß± Step 1: Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "\n",
    "    # ‚úÖ Step 2: Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "\n",
    "    # üîÑ Step 3: Deserialize strings back to Document objects\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71f0b3",
   "metadata": {},
   "source": [
    "## üéØ Final RAG Prompt Template for Answer Generation\n",
    "\n",
    "This is the final instruction template given to the LLM in a RAG setup.\n",
    "\n",
    "It ensures that:\n",
    "- Responses are grounded only in the retrieved context\n",
    "- Irrelevant queries are safely rejected\n",
    "- The tone is professional and brand-aligned\n",
    "- The model avoids hallucinations or personal opinions\n",
    "\n",
    "This template helps keep the system focused, safe, and helpful ‚Äî especially in domain-specific deployments like **Apple mobile customer service**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Define the custom prompt template used in the final RAG stage\n",
    "\n",
    "rag_template = \"\"\"\n",
    "You are a customer service agent for a apple mobile company. \n",
    "You have been given the following information about the customer question and the context.\n",
    "Customer Query: {question}\n",
    "Context: {context}\n",
    "\n",
    "Answer: \n",
    "The answer should be based on the context provided.\n",
    "Your task is to answer the customer question based on the context provided. If the question is not related to the context, please say \"I don't know or Do Not Answer it just say please ask me question related to Apple Mobiles only\".\n",
    "Do not make up any information or provide any personal opinions or experiences.\n",
    "Please answer in a friendly and professional manner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4b3bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nYou are a customer service agent for a apple mobile company. \\nYou have been given the following information about the customer question and the context.\\nCustomer Query: {question}\\nContext: {context}\\n\\nAnswer: \\nThe answer should be based on the context provided.\\nYour task is to answer the customer question based on the context provided. If the question is not related to the context, please say \"I don\\'t know or Do Not Answer it just say please ask me question related to Apple Mobiles only\".\\nDo not make up any information or provide any personal opinions or experiences.\\nPlease answer in a friendly and professional manner.\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "\t(\"system\", \"You are a helpful assistant.\"),\n",
    "\t(\"human\", rag_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(rag_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61db220",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Using Azure OpenAI for Embeddings & Generation\n",
    "\n",
    "First part of the notebook leverages **Azure OpenAI** services to power the RAG pipeline:\n",
    "\n",
    "### üî° Embeddings\n",
    "We use `AzureOpenAIEmbeddings` to convert documents and user queries into dense vector representations. These vectors are stored and queried using a **FAISS vector store**.\n",
    "\n",
    "- Model: Typically `text-embedding-003 small`\n",
    "- Usage: Semantic search to retrieve the most relevant document chunks\n",
    "\n",
    "### ü§ñ Language Model\n",
    "For final response generation, we use `AzureChatOpenAI`, a wrapper around models `gpt-4o-mini-test` hosted on Azure infrastructure.\n",
    "\n",
    "- Model: `gpt-4o-mini-test` (via deployment name)\n",
    "- Input: Original user query + retrieved context\n",
    "- Output: A grounded, domain-specific response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Initialize Azure OpenAI Embeddings\n",
    "open_ai_embeddings = Embeddings.azure_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85f2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    texts,\n",
    "    open_ai_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f08b3444",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ Initialize Azure OpenAI Chat Model (LLM)\n",
    "open_ai_llm = LLMCall.azure_openai()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8aaf4",
   "metadata": {},
   "source": [
    "## üîÑ Multi-Query Chain (Query Expansion)\n",
    "\n",
    "This chain is responsible for generating **multiple diverse sub-queries** from a user's original question.\n",
    "\n",
    "**Why?**  \n",
    "Traditional RAG relies on retrieving documents using a single embedding. If the question is phrased narrowly, relevant information might be missed.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. A prompt instructs the LLM to generate 5 alternate versions of the query.\n",
    "2. The Azure OpenAI model processes the prompt.\n",
    "3. The result is parsed and split into individual questions.\n",
    "4. These questions are then used to perform **multiple document retrievals**, improving recall.\n",
    "\n",
    "This is a key step in **Multi-Query RAG** strategies that boost the quality of retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Multi-Query Chain for Generating Rephrased Questions\n",
    "# This chain uses an LLM to generate 5 diverse sub-queries from a single user question.\n",
    "# It follows this sequence:\n",
    "#   1. m_query_prompt: A prompt template instructing the LLM to generate alternate versions\n",
    "#   2. open_ai_llm: The Azure OpenAI model used for generation\n",
    "#   3. StrOutputParser(): Parses the raw LLM output into a string\n",
    "#   4. lambda x: x.split(\"\\n\"): Splits the result into individual questions (assuming each is newline-separated)\n",
    "\n",
    "multi_query_chain = m_query_prompt| open_ai_llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988cdfe8",
   "metadata": {},
   "source": [
    "## üîç Multi-Query Retrieval Chain\n",
    "\n",
    "After generating multiple rephrasings of the user's question, this chain is responsible for retrieving documents based on each version.\n",
    "\n",
    "### üöÄ Steps:\n",
    "1. **Generate Sub-Queries**  \n",
    "   The original question is transformed into 5 alternate forms using `multi_query_chain`.\n",
    "\n",
    "2. **Parallel Retrieval**  \n",
    "   Each sub-query is passed to the retriever (`retriever.map()`), which fetches the most relevant chunks for each.\n",
    "\n",
    "3. **Deduplication**  \n",
    "   The retrieved results are combined using `get_unique_union` to eliminate duplicate documents.\n",
    "\n",
    "### üéØ Result:\n",
    "A clean, merged list of **diverse but relevant context documents**, ready to be passed to the final generation step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç Multi-Query Retrieval Chain\n",
    "# This chain takes the sub-queries generated by the multi_query_chain and performs the following steps:\n",
    "#   1. multi_query_chain ‚Äî generates 5 rephrased versions of the original question\n",
    "#   2. retriever.map() ‚Äî runs retrieval for each sub-query in parallel\n",
    "#   3. get_unique_union ‚Äî flattens and deduplicates all retrieved chunks into one clean list of documents\n",
    "\n",
    "multi_query_retrieval_chain = multi_query_chain | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3cbfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/39/wh_jffd90vv2lczbpfngvr200000gn/T/ipykernel_31123/170002541.py:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Testing/ Showing if we are getting our questions of not and we are checking the length of unique chunks from it.\n",
    "'''\n",
    "\n",
    "# Retrieve\n",
    "question = \"Is there a warranty on the phone?\"\n",
    "docs = multi_query_retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc6cfce",
   "metadata": {},
   "source": [
    "## üß† Final RAG Chain ‚Äì Context + Answer Generation\n",
    "\n",
    "This is the last stage in the **Multi-Query RAG** pipeline.  \n",
    "It takes the original question and the merged context retrieved from sub-queries and generates a final answer.\n",
    "\n",
    "### üì¶ Components:\n",
    "\n",
    "- **`multi_query_retrieval_chain`**: Provides a deduplicated list of relevant chunks\n",
    "- **`itemgetter(\"question\")`**: Selects the original user query for use in the prompt\n",
    "- **`rag_prompt`**: Structured template that instructs the model how to use the context\n",
    "- **`open_ai_llm`**: Azure-hosted GPT model for generating the response\n",
    "- **`StrOutputParser()`**: Cleans up the output and returns the final answer as a string\n",
    "\n",
    "This chain ensures that the final answer is:\n",
    "- Grounded in retrieved documents\n",
    "- Context-aware\n",
    "- Polite, accurate, and brand-aligned (as per the prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637693c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Final RAG Chain ‚Äì Answer Generation Step\n",
    "# This chain combines the user's original question with the deduplicated retrieved context,\n",
    "# feeds it into the final prompt, and uses the LLM to generate a grounded answer.\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "\t    \"context\": multi_query_retrieval_chain,    # üîç Retrieved and deduplicated context from multi-query RAG\n",
    "        \"question\": itemgetter(\"question\")         # üí¨ Original user question (unchanged)\n",
    "    } \n",
    "    | rag_prompt                                   # üìù Custom prompt template guiding the LLM\n",
    "    | open_ai_llm                                  # ü§ñ Azure OpenAI model for response generation\n",
    "    | StrOutputParser()                            # üßæ Parse the final answer as plain text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7f038a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Answer: Yes, there is a warranty on the phone. Apple offers a One-Year Limited Warranty that covers defects in materials and workmanship for one year from the date of original retail purchase. However, this warranty does not cover normal wear and tear or damage caused by accident or abuse. If you need service, you can call Apple or visit an Apple Store or an Apple Authorized Service Provider. For more detailed information, you can visit apple.com/legal/warranty.\n"
     ]
    }
   ],
   "source": [
    "response = final_rag_chain.invoke({\"question\":question})\n",
    "\n",
    "print('üì¶ Answer:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25385c7f",
   "metadata": {},
   "source": [
    "## ü§ó Using Hugging Face for Embeddings & Generation\n",
    "\n",
    "This RAG pipeline is powered by **open-source Hugging Face models** for both:\n",
    "\n",
    "---\n",
    "\n",
    "### üî° Embeddings\n",
    "We use `HuggingFaceEmbeddings` (e.g., from `sentence-transformers`) to convert documents and queries into dense vector representations.\n",
    "\n",
    "- üì¶ Common model: `sentence-transformers/all-mpnet-base-v2`\n",
    "- ‚öôÔ∏è Device support: `cpu` or `cuda`\n",
    "- üßÆ These vectors are used for semantic search via FAISS\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Language Model\n",
    "Text generation is handled using Hugging Face LLMs (e.g., `gemma-2b-it`, `mistral`, `phi`, etc.), loaded via `AutoModelForCausalLM`.\n",
    "\n",
    "- üí¨ Accepts original query + retrieved context to generate a grounded response\n",
    "- üîÅ Wrapped using `HuggingFacePipeline` for LangChain compatibility\n",
    "- üéõÔ∏è Supports advanced tuning (`temperature`, `top_p`, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### üí∏ Cost-Effective & Open Source\n",
    "\n",
    "Using Hugging Face models offers major advantages:\n",
    "\n",
    "- ‚úÖ **Free and open-source**: No pay-per-token costs\n",
    "- ‚úÖ **Runs locally**: No API keys, no vendor lock-in\n",
    "- ‚úÖ **Budget-friendly**: Great for research, startups, and personal projects\n",
    "- ‚úÖ **Customizable**: You can fine-tune or quantize for your specific use case\n",
    "\n",
    "> üí° Hugging Face models help **save burden on the pocket** while offering full control and flexibility for local, private, or large-scale deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851c7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_embeddings = Embeddings.huggingface()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b37287b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    texts,\n",
    "    huggingface_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62cf5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4fa917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Same Multi-query template as above \n",
    "# but as we mentioned above but here we are using huggingface embeddings and huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c223a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_llm = LLMCall.huggingface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_chain = m_query_prompt | huggingface_llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "multi_query_retrieval_chain = multi_query_chain | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6266f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": multi_query_retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | rag_prompt\n",
    "    | huggingface_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b29a084b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Answer: Yes, there is a warranty on the phone. Apple offers a one-year limited warranty that covers defects in materials and workmanship for the included hardware product and accessories from the date of original retail purchase. However, this warranty does not cover normal wear and tear or damage caused by accident or abuse. If you need to obtain service, you can call Apple or visit an Apple Store or an Apple Authorized Service Provider. For more detailed information, you can visit apple.com/legal/warranty. If you have any further questions, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "response = final_rag_chain.invoke({\"question\":question})\n",
    "\n",
    "print('üì¶ Answer:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e0a48",
   "metadata": {},
   "source": [
    "## ü¶ô Using Ollama for Local LLM Inference\n",
    "\n",
    "In this pipeline, we're combining:\n",
    "\n",
    "- ü§ó **Hugging Face Embeddings** (for vector similarity search)\n",
    "- ü¶ô **Ollama** (for fast, local LLM generation)\n",
    "\n",
    "---\n",
    "\n",
    "### üî° Embeddings\n",
    "We continue to use `HuggingFaceEmbeddings` (e.g., `all-mpnet-base-v2`) for encoding both:\n",
    "- Document chunks\n",
    "- User queries\n",
    "\n",
    "These embeddings are stored and queried using **FAISS**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Language Model via Ollama\n",
    "\n",
    "Instead of using models from OpenAI or Hugging Face Hub, we run a **locally downloaded LLM** using [Ollama](https://ollama.com/).\n",
    "\n",
    "- ‚úÖ In this example, we use the **`llama3`** model (v3.2)\n",
    "- ‚úÖ Ollama runs the model **on your local machine** using GPU or CPU\n",
    "- ‚úÖ It's fully private and doesn't incur per-token costs\n",
    "\n",
    "---\n",
    "\n",
    "### üì• To Run with a Different Model\n",
    "\n",
    "Ollama supports various models like `mistral`, `gemma`, `phi`, etc.  \n",
    "To download and use another model:\n",
    "\n",
    "```bash\n",
    "ollama run <model_name>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf767166",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_llm = LLMCall.chat_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24978fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_chain = m_query_prompt | ollama_llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "multi_query_retrieval_chain = multi_query_chain | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc7138d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": multi_query_retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | rag_prompt\n",
    "    | ollama_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a2503c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Answer: Yes, there is a warranty on the phone. According to the information provided, Apple offers a one-year limited warranty on the hardware product and accessories against defects in materials and workmanship from the date of original retail purchase. You can find more detailed information on obtaining service and the full terms of the warranty at apple.com/legal/warranty and support.apple.com.\n"
     ]
    }
   ],
   "source": [
    "response = final_rag_chain.invoke({\"question\":question})\n",
    "\n",
    "print('üì¶ Answer:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6befae8",
   "metadata": {},
   "source": [
    "Here‚Äôs a polished, notebook-ready **Markdown cell** for explaining your use of the **Groq Inference API**, including how to switch models and where it connects in your code:\n",
    "\n",
    "---\n",
    "\n",
    "```markdown\n",
    "## ‚ö° Using Groq Inference API\n",
    "\n",
    "In this example, we're utilizing the **Groq Inference API** to power our language generation step with **ultra-fast LLM performance**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîë API Key Access\n",
    "\n",
    "To use Groq, you'll need an API key from [console.groq.com](https://console.groq.com).  \n",
    "This key allows access to hosted LLMs via their **low-latency inference platform**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Model in Use\n",
    "\n",
    "We're currently using:\n",
    "\n",
    "```\n",
    "llama-3.3-70b-versatile\n",
    "```\n",
    "\n",
    "Also known as:\n",
    "\n",
    "```\n",
    "llama-3.3\n",
    "```\n",
    "\n",
    "This model is known for its large context window and versatility across general-purpose tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Switching Models\n",
    "\n",
    "If you'd like to use a different model from Groq's lineup (like `mixtral-8x7b`, `gemma-7b-it`, etc.):\n",
    "\n",
    "- ‚úÖ Option 1: Pass the model name into the function call directly:\n",
    "  \n",
    "  ```python\n",
    "  llm = LLMCall.groq(model=\"mixtral-8x7b\")\n",
    "  ```\n",
    "\n",
    "- ‚úÖ Option 2: Update it in your custom logic:\n",
    "  \n",
    "  Modify the model name inside the `chat_groq()` method of your `LLMCall` class in `llm_call.py`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Why Use Groq?\n",
    "\n",
    "- üöÄ Extremely fast inference speeds\n",
    "- ‚úÖ No need to host your own models\n",
    "- üí° Ideal for low-latency, production-grade deployments\n",
    "\n",
    "> üí° You're still using local/affordable embeddings (e.g. Hugging Face), but generation is powered by high-speed, remote Groq-hosted LLMs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e210136",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = LLMCall.chat_groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2319ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_chain = m_query_prompt | groq_llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    "multi_query_retrieval_chain = multi_query_chain | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "615e774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": multi_query_retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | rag_prompt\n",
    "    | groq_llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04c8a78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Answer: Yes, there is a warranty on the phone. According to the Apple One-Year Limited Warranty Summary, Apple warrants the included hardware product and accessories against defects in materials and workmanship for one year from the date of original retail purchase. You can find more detailed information on obtaining service at apple.com/legal/warranty and support.apple.com.\n"
     ]
    }
   ],
   "source": [
    "response = final_rag_chain.invoke({\"question\":question})\n",
    "\n",
    "print('üì¶ Answer:', response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3db2a5",
   "metadata": {},
   "source": [
    "<!-- Font Awesome CDN (Add in <head> if not already included) -->\n",
    "<link\n",
    "  rel=\"stylesheet\" \n",
    "  href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css\"\n",
    "/>\n",
    "\n",
    "<!-- Social Footer Section -->\n",
    "<div style=\"\n",
    "  background-color:rgb(199, 195, 195);\n",
    "  padding: 40px 30px;\n",
    "  border-radius: 20px;\n",
    "  box-shadow: 0 4px 12px rgba(0,0,0,0.08);\n",
    "  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "  font-size: 18px;\n",
    "  max-width: 900px;\n",
    "  margin: 60px auto 30px;\n",
    "  text-align: center;\n",
    "  color: #444;\n",
    "\">\n",
    "<!-- End of Notebook Note -->\n",
    "  <h2 style=\"margin-bottom: 10px;\">üìò End of Notebook</h2>\n",
    "  <p style=\"color: #666; font-size: 14px;\">\n",
    "    Thank you for exploring! Feel free to connect via the links below.\n",
    "  </p>\n",
    "\n",
    "  <!-- Social Icons -->\n",
    "<div style=\"\n",
    "  display: flex;\n",
    "  gap: 25px;\n",
    "  align-items: center;\n",
    "  flex-wrap: wrap;\n",
    "  justify-content: center;\n",
    "  margin-bottom: 25px;\n",
    "\">\n",
    "  <!-- LinkedIn -->\n",
    "  <a href=\"https://www.linkedin.com/in/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #0077b5;\">\n",
    "    <i class=\"fab fa-linkedin fa-lg\"></i> LinkedIn\n",
    "  </a>\n",
    "\n",
    "  <!-- GitHub -->\n",
    "  <a href=\"https://github.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #333;\">\n",
    "    <i class=\"fab fa-github fa-lg\"></i> GitHub\n",
    "  </a>\n",
    "\n",
    "  <!-- Instagram -->\n",
    "  <a href=\"https://www.instagram.com/data.scientist_chirag\" target=\"_blank\" style=\"text-decoration: none; color: #E1306C;\">\n",
    "    <i class=\"fab fa-instagram fa-lg\"></i> Instagram\n",
    "  </a>\n",
    "\n",
    "  <!-- Email -->\n",
    "  <a href=\"mailto:devchirag27@gmail.com\" style=\"text-decoration: none; color: #D44638;\">\n",
    "    <i class=\"fas fa-envelope fa-lg\"></i> Email\n",
    "  </a>\n",
    "\n",
    "  <!-- X (Twitter) -->\n",
    "  <a href=\"https://x.com/ChiragB254\" target=\"_blank\" style=\"text-decoration: none; color: #000;\">\n",
    "    <i class=\"fab fa-x-twitter fa-lg\"></i> X.com\n",
    "  </a>\n",
    "  </div>\n",
    "\n",
    "  <p style=\"font-size: 13px; color: black; font-style: italic; margin-top: 8px;\">\n",
    "    <strong>Made with ‚ù§Ô∏è by Chirag Bansal</strong>\n",
    "  </p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
